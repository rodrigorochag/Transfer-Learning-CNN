{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN+HOt1TgqikQHgf2c2iXew",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rodrigorochag/Transfer-Learning-CNN/blob/main/transfer_learning_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZYmVSD_L7iMH"
      },
      "outputs": [],
      "source": [
        "import os, time\n",
        "from tempfile import TemporaryDirectory\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader, random_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Kaggle library\n",
        "!pip install -q kaggle\n",
        "\n",
        "# Set Kaggle credentials\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Replace the following with your Kaggle username and API key\n",
        "\n",
        "kaggle_info = {\n",
        "    \"username\": \"user_kaggle\",\n",
        "    \"key\": \"api_key\"\n",
        "}\n",
        "# Save Kaggle credentials to a JSON file\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w\") as file:\n",
        "    json.dump(kaggle_info, file)\n",
        "\n",
        "# Change the permissions of the file\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "# Download the dataset\n",
        "!kaggle datasets download -d misrakahmed/vegetable-image-dataset\n",
        "# Unzip the dataset\n",
        "!unzip -q /content/vegetable-image-dataset.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6Wqv92U7_GC",
        "outputId": "730f7aad-ee32-4fb2-b043-c9fd2839c548"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/misrakahmed/vegetable-image-dataset\n",
            "License(s): CC-BY-SA-4.0\n",
            "Downloading vegetable-image-dataset.zip to /content\n",
            "100% 533M/534M [00:30<00:00, 20.1MB/s]\n",
            "100% 534M/534M [00:30<00:00, 18.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ],
      "metadata": {
        "id": "lYcePVGM9WfQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.ImageFolder('/content/Vegetable Images/train')\n",
        "\n",
        "train_size = int(0.8 * len(dataset))  # 80% for training\n",
        "val_size = len(dataset) - train_size  # 20% for validation\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "\n",
        "train_dataset.dataset.transform = data_transforms['train']\n",
        "val_dataset.dataset.transform = data_transforms['val']"
      ],
      "metadata": {
        "id": "X7bD5ieF-ZXv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders = {\n",
        "    name: torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=4,\n",
        "        shuffle=True,\n",
        "        num_workers=4\n",
        "    ) for name, dataset in zip([\"train\", \"val\"], [train_dataset, val_dataset])\n",
        "}\n",
        "\n",
        "class_names = train_dataset.dataset.classes\n",
        "dataset_sizes = { name: len(dataset) for name, dataset in zip([\"train\", \"val\"], [train_dataset, val_dataset]) }\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class_names, dataset_sizes, device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZPz_7dd_anr",
        "outputId": "fc389fb6-3a49-4863-b3e8-3dca0538be4f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Bean',\n",
              "  'Bitter_Gourd',\n",
              "  'Bottle_Gourd',\n",
              "  'Brinjal',\n",
              "  'Broccoli',\n",
              "  'Cabbage',\n",
              "  'Capsicum',\n",
              "  'Carrot',\n",
              "  'Cauliflower',\n",
              "  'Cucumber',\n",
              "  'Papaya',\n",
              "  'Potato',\n",
              "  'Pumpkin',\n",
              "  'Radish',\n",
              "  'Tomato'],\n",
              " {'train': 12000, 'val': 3000},\n",
              " device(type='cuda', index=0))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    # Iterate over data.\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # backward + optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    return running_loss, running_corrects\n",
        "\n",
        "\n",
        "def validate_epoch(model, dataloader, criterion, device):\n",
        "    model.eval()   # Set model to evaluate mode\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    # Iterate over data.\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # forward\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    return running_loss, running_corrects\n",
        "\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=3):\n",
        "    since = time.time()\n",
        "\n",
        "    # Create a temporary directory to save training checkpoints\n",
        "    with TemporaryDirectory() as tempdir:\n",
        "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
        "        torch.save(model.state_dict(), best_model_params_path)\n",
        "        best_acc = 0.0\n",
        "        history = []\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # Each epoch has a training and validation phase\n",
        "            train_loss, train_corrects = train_epoch(model, dataloaders['train'], criterion, optimizer, device)\n",
        "            scheduler.step()\n",
        "            val_loss, val_corrects = validate_epoch(model, dataloaders['val'], criterion, device)\n",
        "\n",
        "            train_loss /= dataset_sizes['train']\n",
        "            train_acc = train_corrects.double() / dataset_sizes['train']\n",
        "            val_loss /= dataset_sizes['val']\n",
        "            val_acc = val_corrects.double() / dataset_sizes['val']\n",
        "\n",
        "            history.append([train_acc, val_acc, train_loss, val_loss])\n",
        "            print(f'Epoch {epoch}/{num_epochs - 1}: '\n",
        "                  f'Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, '\n",
        "                  f'Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}')\n",
        "\n",
        "            # deep copy the model\n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "        print(f'Best val Acc: {best_acc:.4f}')\n",
        "\n",
        "        # load best model weights\n",
        "        model.load_state_dict(torch.load(best_model_params_path))\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "_Rfo42akAQjd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_orig = torchvision.models.convnext_base(weights='IMAGENET1K_V1')\n",
        "model_orig.classifier[2] = torch.nn.Linear(\n",
        "    in_features=model_orig.classifier[2].in_features,\n",
        "    out_features=len(class_names)\n",
        ")\n",
        "model_orig = model_orig.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9L3-ws1Aazz",
        "outputId": "16ffea2a-cc72-47f2-baeb-005698a4346d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/convnext_base-6075fbad.pth\" to /root/.cache/torch/hub/checkpoints/convnext_base-6075fbad.pth\n",
            "100%|██████████| 338M/338M [00:02<00:00, 129MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft = models.convnext_base(weights='IMAGENET1K_V1')\n",
        "model_ft.classifier[2] = torch.nn.Linear(\n",
        "    in_features=model_ft.classifier[2].in_features,\n",
        "    out_features=len(class_names)\n",
        ")\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.0005)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=6, gamma=0.1)"
      ],
      "metadata": {
        "id": "ab8YYjftAi2e"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft, model_ft_history = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=13)"
      ],
      "metadata": {
        "id": "B_JnShTyAmwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_conv = torchvision.models.convnext_base(weights='IMAGENET1K_V1')\n",
        "for param in model_conv.parameters(): param.requires_grad = False\n",
        "model_conv.classifier[2] = torch.nn.Linear(\n",
        "    in_features=model_conv.classifier[2].in_features,\n",
        "    out_features=len(class_names)\n",
        ")\n",
        "model_conv = model_conv.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_conv = optim.SGD(model_conv.classifier[2].parameters(), lr=0.0005)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=6, gamma=0.1)"
      ],
      "metadata": {
        "id": "w5_hT3eBAqW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_conv, model_conv_history = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=13)"
      ],
      "metadata": {
        "id": "27q2U7O-AtPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft_history = pd.DataFrame(model_ft_history, columns=[\"Train Accuracy\", \"Validation Accuracy\", \"Train Loss\", \"Validation Loss\"])\n",
        "model_conv_history = pd.DataFrame(model_conv_history, columns=[\"Train Accuracy\", \"Validation Accuracy\", \"Train Loss\", \"Validation Loss\"])"
      ],
      "metadata": {
        "id": "dceKU8C_AzgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_of_subplots = len(model_ft_history.columns)\n",
        "\n",
        "fig, axs = plt.subplots(nrows=2, ncols=2, sharex=True)\n",
        "\n",
        "for i, title, ax in zip(range(num_of_subplots), model_ft_history.columns, [i for x in axs for i in x]):\n",
        "    ax.plot(model_ft_history.iloc[:, i], label='Full fine-tuning')\n",
        "    ax.plot(model_conv_history.iloc[:, i], label='Partial fine-tuning')\n",
        "    ax.set_xticks(range(13))\n",
        "    ax.set_xticklabels((\"1\", \" \", \"3\", \" \", \"5\", \" \", \"7\", \" \", \"9\", \" \", \"11\", \" \", \"13\"))\n",
        "    ax.xaxis.set_ticks_position('none')\n",
        "    ax.yaxis.set_ticks_position('none')\n",
        "    if i > 1:  ax.set_xlabel('Epoch')\n",
        "    if i == 0: ax.set_ylabel('Accuracy')\n",
        "    if i == 2: ax.set_ylabel('Loss')\n",
        "    ax.set_title(title)\n",
        "    ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "32bNJN1LA0As"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_model_predictions(model, img_path):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "\n",
        "    img = Image.open(img_path)\n",
        "    img = data_transforms['val'](img)\n",
        "    img = img.unsqueeze(0)\n",
        "    img = img.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        model.train(mode=was_training)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "vcaznUSgA6OR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, [ax1, ax2, ax3] = plt.subplots(nrows=1, ncols=3, figsize=(15, 3))\n",
        "\n",
        "img = cv2.imread(\"/content/Vegetable Images/train/Bean/0026.jpg\")\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "ax1.imshow(img)\n",
        "ax1.set_title(\"Test Image\")\n",
        "\n",
        "original_preds = visualize_model_predictions(model_orig, \"/content/Vegetable Images/train/Bean/0026.jpg\")\n",
        "ax2 = pd.Series(original_preds.data.cpu().numpy().tolist()[0]).plot(kind='bar', ax=ax2)\n",
        "ax2.set_xticklabels(class_names, rotation=60)\n",
        "ax2.axhline(y=0, color='green', linestyle='--')\n",
        "ax2.set_title(\"Predictions before fine-tuning\")\n",
        "\n",
        "finetuned_preds = visualize_model_predictions(model_ft, \"/content/Vegetable Images/train/Bean/0026.jpg\")\n",
        "pd.Series(finetuned_preds.data.cpu().numpy().tolist()[0]).plot(kind='bar', ax=ax3)\n",
        "ax3.set_xticklabels(class_names, rotation=60)\n",
        "ax3.axhline(y=0, color='green', linestyle='--')\n",
        "ax3.set_title(\"Predictions after fine-tuning\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rT1tlY57A9Lp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}